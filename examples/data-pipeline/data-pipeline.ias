package "data-pipeline" version "0.1.0" lang "1.0"

prompt "etl" {
  content "You are a data engineering assistant. Help users extract\n           data from sources, transform it according to rules, and\n           load it into target systems. Always validate data quality\n           before loading."
}

skill "extract" {
  description "Extract data from a source system"
  input {
    source string required
  }
  output {
    raw_data string
  }
  execution command "data-extract"
}

skill "transform" {
  description "Transform data according to mapping rules"
  input {
    data string required
  }
  output {
    transformed string
  }
  execution command "data-transform"
}

skill "load" {
  description "Load data into the target database"
  input {
    data string required
  }
  output {
    rows_inserted string
  }
  execution command "data-load"
}

skill "validate" {
  description "Run data quality checks"
  input {
    data string required
  }
  output {
    report string
  }
  execution command "data-validate"
}

agent "etl-bot" {
  uses prompt "etl"
  uses skill "extract"
  uses skill "transform"
  uses skill "load"
  uses skill "validate"
  model "claude-sonnet-4-20250514"
}

secret "db-connection" {
  env(DATABASE_URL)
}

secret "source-api-key" {
  env(SOURCE_API_KEY)
}

policy "production-safety" {
  deny model claude-haiku-latest
  require secret db-connection
}

environment "dev" {
  agent "etl-bot" {
    model "claude-haiku-latest"
  }
}

environment "staging" {
  agent "etl-bot" {
    model "claude-sonnet-4-20250514"
  }
}

environment "prod" {
  agent "etl-bot" {
    model "claude-sonnet-4-20250514"
  }
}

binding "local" adapter "local-mcp" {
  default true
}

binding "compose" adapter "docker-compose" {
  output_dir "./pipeline-deploy"
}
